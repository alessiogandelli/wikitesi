\chapter{Infrastacture}
All the code in available on Github \url{https://github.com/WikiCommunityHealth/wikimedia-revert},
there is an onrganiziation called WikiCommunityHealth where each team member commits its
contribution to the project. For this dataprocessing we used python since is the best option to
handle this amount of data. The data is currently stored in the Unitn servers of Cricca group.

\paragraph*{Multi Language}
All the dataset computed are the results of several python scripts launched singularly. All the work
has been done using the italian wikipedia as example. Automatizing the process allow us to run all
the scripts in different languages. For achieving this automatation we used a bash script which
takes the language as parameters e.g \textit{./generate\_dataset it} takes the data from the WikiMedia history
dumps in italian, create a folder "it" and all the subfolders needed and generate the dataset in the
right place. the only requirements is that the dump is already been downloaded. 

\paragraph*{Folder structure}
Here the folder structure of how the data is stored :  

\dirtree{%
.1 ita.
.2 chains.
.3 user.
.4 wars.json.
.4 monthly.tsv.
.3 page.
.4 wars.json.
.4 monthly.tsv.
.3 page\_reg.
.4 wars.json.
.2 group.
.3 user.
.4 mutuals.tsv.
.4 reverts.tsv.
.4 all.tsv.
.3 page.
.4 mutuals.tsv.
.4 reverts.tsv.
.4 all.tsv.
}

\paragraph*{Bash Script}
The main code written is in python but for some of the task we decided that was better using a bash
script, in particular to automatizing process like downloading the Wikimedia History Dumps or the generation of 



\paragraph{style}
wide use of dictionary 