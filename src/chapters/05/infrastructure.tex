\chapter{Infrastructure}

All the code is available on Github
\footnote{\url{https://github.com/WikiCommunityHealth/wikimedia-revert}} where there is an
organization called WikiCommunityHealth in which every team member gives its contribution to the
project. For the data processing, we used python since it's the best option to handle such an amount of
data. The data is currently stored in the Unitn servers of the Cricca group.
\paragraph*{Multi Language}
All the computed datasets are the results of several python scripts launched one by one. All the
work has been done using Italian Wikipedia as an example. Automatizing the process allows us to run
all the scripts in different languages without further effort. In order to achieve this automation we
used a bash script that takes the language as parameters e.g \textit{./generate\_dataset it} takes
the data from the Wikimedia History Dumps in Italian, then creates a folder called "it" and all the required subfolders
and then it generates the dataset in the right location. The only requirement is that the dump
must have already been downloaded. 


\section{Workflow}

\paragraph{File}
To compute the datasets, since we have to process huge amounts of data, we decided to use a
simple style of programming without using complex libraries. We used dictionaries as an example of
one of the most efficient data structures. Even if the computed dataset were different we always used
the same structure of the code, that consisted in a few steps, the program reads the compressed
Wikimedia History Dumps line by line and for each line :
\begin{enumerate}
    \item Parse from the dataset the pieces of information.
    \item Insert in dictionaries the information we want to save.
    \item Check if the page id is different from the previous one, if this is true it means that the
    page is finished so we can process it and initialize all the variables for a new page.
    \item If the page is not finished, we check if the month is finished and similarly to the page
    we process the information we gathered since we want to save an entry for each month.
    \item If neither the page nor the month is finished we can check if this revision is reverting the
    previous one or doing the computation we needed in that specific file.
\end{enumerate}

\section{Repository}
To handle a big project it is always important to organize in the best way possible the repository to avoid
confusion while browsing. Also the naming of the file is important.
\paragraph{Folder}
This is the structure of the folder where the code is organized, the bash folder contains all the used bash
scripts. The utility one has all the python files that were used to check various things, for example,
some files let us extract from the data about a specific pagefrom the dataset. The main
folder has all the files concerning the computation and analysis of the datasets, while the
database one has the script to upload the files on the database for the interactive dashboard.

\dirtree{%
.1 bash.
.1 main.
.2 analyze.
.2 create.
.2 database.
.1 utility.
}
\paragraph{Naming}
All the files in the main directory follow strict naming rules. The format is : \\
\texttt{type\_class\_aggregation\_name\_month\_format.py}
\begin{itemize}
    \item \textit{type} the file can be one that creates (c) or that analyses (a) a dataset
    \item \textit{class} since this work could be divided into 2 different sections, Chain and Group, we used
    them as the main identifier to classify the files. There is also a generic class used when the computed data
    wasn't neither a chain nor a group.
    \item \textit{aggregation} if the file is cathegorized by user or by page 
    \item \textit{name} the name of the metrics it computes
    \item \textit{month} (optional) if the file is cathegorized by month 
    \item \textit{format} In create files the format of the output file is written directly in the file
    name in order to quickly understand what kind of data it handles (TSV or JSON)
\end{itemize}

\section{Data}
A lot of different files were created in the process, so they must be well organized in order to retrieve them
without errors. There is a folder for each class, chains and groups, and for both of them there is
a folder for page and one for users.

\paragraph*{Folder structure}
Here is the folder structure of how the data was stored :  

\dirtree{%
.1 ita.
.2 chains.
.3 user.
.4 wars.json.
.4 monthly.tsv.
.3 page.
.4 wars.json.
.4 monthly.tsv.
.3 page\_reg.
.4 wars.json.
.2 group.
.3 user.
.4 mutuals.tsv.
.4 reverts.tsv.
.4 all.tsv.
.3 page.
.4 mutuals.tsv.
.4 reverts.tsv.
.4 all.tsv.
}

\paragraph*{Bash Script}
The main code is written in python but for some of the tasks we decided that using a bash
script was a better idea, in particular in order to automatize processes like downloading the Wikimedia History Dumps or
generating the datasets.





