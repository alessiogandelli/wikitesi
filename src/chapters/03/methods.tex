\chapter{Methods}

Considering the huge size of the dataset and the fact that a large portion of its content was
useless, smaller datasets have been computed with the aim of expediting the analysis even for future
usages. The analysis was made based on the computed datasets. These datasets can be computed for
every language thanks to a bash script, in this way a multilingual analysis on the most
controversial topics can be conducted in different locations.


\bigskip



\tikzstyle{square} = [rectangle, rounded corners, minimum width=3cm, minimum height=1cm,text centered,text width=3cm, draw=black, fill=blue!20]
\tikzstyle{arrow} = [thick,->,>=stealth]

\begin{tikzpicture}[node distance=4cm]
    \node (dataset) [square, xshift=4cm] {MediaWiki History Dumps};
    \node (computed) [square, right of=dataset, xshift=2cm] {Computed Datasets};
    \node (anal) [square, right of=computed, xshift=2cm] {Analysis};

    \draw [arrow] (dataset) --node[anchor=south] {compute}(computed);
    \draw [arrow] (computed) -- node[anchor=south]{analyze}(anal);

\end{tikzpicture}

\bigskip

\section{Computed Dataset}
After the first skimming, only the revisions involving a revert were saved. This dataset, whose
schema is the same as the MediaWiki History Dumps, has been sorted by both page and timestamp, and
thanks to this screening, the size is now $\sim 10\%$ of the original. In order to achieve this result,
the compressed dataset has been decompressed line by line on the fly and only the entries we were
interested in have been saved in a file. Therefore only a small amount of RAM and disk space is
required since all data is compressed. For the sorting part the most optimized way to sort a file,
which is Unix sort, was used.  

From this filtered dataset several smaller datasets have been computed, and these can be divided into two modules: 

\begin{itemize}
    \item Chains:  the focus was on detecting revert chains in the pages 
    \item Group:  the focus was posed on the number of reverts that users made or received based on the groups they belong to (admin, registered, anonymous).
\end{itemize}

\subsection{Chains}
The data concerning revert chains have been computed from the compressed filtered dataset. Every
time the filtered dataset was analyzed, it was read line by line and only the interesting
pieces of information were saved. The output is a JSON file, in which every page corresponds to a JSON object.
A list of chains and some statistics have been saved for each page. Every chain has a start and an end date, a
list of revisions, and the name of the involved users. The resulting dataset is way smaller than the initial one so it is
possible to browse it in only a few seconds.\\


In order to identify a chain, we used a function, called \textit{simple\_chains}, that differs from
another one, called \textit{complex\_chains} because it identifies a chain of revert only
considering contiguous reverts. We decided to use the simple one because we were only interested in
those chains that occur in a short time span, since there is where most of the discussions take place.
If more than 50\% of users involved in a chain were bots the chain was excluded. There are two
versions of this dataset, one of which considers anonymous users and one that does not. \\

In the schema below there are all the fields in a page object. 
\begin{verbatim}
    {
        "title": "Loligo_vulgaris", 
        "chains": 
        [{
            "revisions": ["113715375", "113715381", "113715393"], 
            "users": {"62.18.117.244": "", "Leo0428": "17181"}, 
            "len": 3, 
            "start": "2020-06-15 22:16:23.0", 
            "end": "2020-06-15 22:17:38.0"
        }], 
        "n_chains": 1, 
        "n_reverts_in_chains": 3, 
        "n_reverts": 38
        "mean": 3.0, 
        "longest": 3, 
        "G": 0,
        "M": 0, 
        "lengths": {"3": 1}
    }
    
\end{verbatim}


The user object is very similar, but it is calculated with another procedure. All the data we needed
was stored in the JSON pages. By analyzing that file all the chains in which a user has been
involved can be extracted, and then statistics can be calculated in a similar way as for pages.
Using this dataset it can be computed 10 times faster.  

The only difference is that the M field is missing because it is only related to a page, while the G field
can be computed on a user considering every chain in which it is the author of at least one revision.\\

The dataset was also computed monthly for both users and pages, the schema is simpler than the
JSON one and this allows us to save it in a TSV using only one row for each month. Instead of saving
all the data regarding the chain, only the numbers of chains longer more than 5, 7, 9 were saved. In
Table \ref{table:chainsPagemonth} there is a sample page entry. In order to do this, the JSON
dataset has been processed one page (or user) at a time, after it was divided by month. The chains
were counted per month basing on the start date of the chain.   

\begin{table}[H]
    \centering
    \ra{1.2}
    \begin{tabularx}{\columnwidth}{@{}Xcccccccccc@{}}
        \midrule
        \textbf{title} & \textbf{year\_month} & \textbf{n of chain} & \textbf{n rev in chain} & \textbf{mean} & \textbf{longest} & \textbf{$\geq$ 5} & \textbf{$\geq$ 7} & \textbf{$\geq$ 9} & \textbf{G}\\ \toprule
        Franz\_Kafka & 2018-11 & 11 & 113 & 10.3 & 51 & 4 & 4 & 3 & 0\\
        
        \bottomrule
    \end{tabularx}
    
    \caption{Entry of the mothly TSV \label{table:chainsPagemonth}}
\end{table}


\subsection{Group}
Another interesting part of this study was focusing on the category a user belongs. Thanks to this
we were able to track the habits of the users, and this can allow us to understand, for example, if
someone stopped editing Wikipedia after several reverts from admins. Detecting these kinds of
patterns is useful for community health. The groups to which users can belong are: 


\begin{itemize}
    \item Admin (sysop): can perform certain actions like blocking users and editing protected pages, 
    \item Registered: are logged in at the time of the edit, 
    \item Anonymous: are not logged in and their username is their IP address(it is not possible to match an IP with a user
        because the IP can change over time).
\end{itemize}

The datasets computed are both for pages and users: 
\paragraph*{Pages} 
For each page, there are two topics of investigation: reverts and mutual reverts. An entry of the
dataset is a page-month containing the number of reverts and mutual reverts made on the page
divided by group. This can be helpful, for example, to detect pages where admins are more active and
this could be a sign that something is wrong with the page.



The notation \textit{adm\_reg} in Table \ref{table:revertpage} refers to the number of admin that performed a
revert to a registered user (similarly with \textit{adm\_adm, reg\_adm, reg\_reg} ).\\

The notation \textit{mut\_ra} in the Table \ref{table:mutualpage} refers to the number of mutual
reverts where the pair is composed by a registered user and an admin. The order of the user does not
matter, in fact, there is no \textit{mut\_ar} that would have the same value.\\


Since the focus was on experienced users, only pairs involving registered and admins were computed.
For having an idea of the volume of the reverts made by anonymous we saved the number of reverts
that were made by both anonymous (\textit{anon}) and not anonymous (\textit{not\_anon}).

To compute these metrics simple variables have been used. They have been incremented, if
necessary, at each entry of the dataset and they have been initialized each time a new page 
started. For both users and pages, we have discarded edits that have been marked as vandalism and
edits made by bots.

\begin{table}[H]
    \centering
    \ra{1.2}
    \begin{tabularx}{\columnwidth}{@{}Xcccccccccc@{}}
        \midrule
        \textbf{id} & \textbf{page} & \textbf{year\_month} & \textbf{adm\_adm} & \textbf{adm\_reg} & \textbf{reg\_adm} & \textbf{reg\_reg} & \textbf{anon} & \textbf{not\_anon}\\ \toprule
        1 & AS\_Roma & 2020-10 & 14 & 245 & 36 & 308 & 1493 & 603 \\
        
         \bottomrule
    \end{tabularx}
    
    \caption{Entry of the revert page TSV. \label{table:revertpage}}
\end{table}

Mutual reverts are not as easy to compute as reverts. We need to store information of the whole page
in order to correctly detect all the mutual reverts.

The most efficient way to save such information is using dictionaries. For each reverter has been
saved the list of users who reverted. At the time of processing the page the saved information
allowed us to compute mutual revert pairs.

\begin{table}[H]
    \centering
    \ra{1.2}
    \begin{tabularx}{\columnwidth}{@{}Xcccccccc@{}}
        \midrule
        \textbf{id} & \textbf{page} & \textbf{year\_month} & \textbf{M}& \textbf{mut\_aa} & \textbf{mut\_ra}  & \textbf{mut\_rr} & \textbf{anon} & \textbf{not\_anon}\\ \toprule
        1 & Giorgio\_Napolitano & 2020-07 & 7681159 & 0 & 4  & 3 & 61 & 7 \\
         \bottomrule
    \end{tabularx}
    
    \caption{Entry of the mutual revert page TSV. \label{table:mutualpage}}
\end{table}

\paragraph*{User}
It is useful also to have the data aggregated by user. Reverts data can be retrieved from the
filtered dataset sorted by timestamp. The data about reverts is gathered and processed month by
month. We store, for each user-month, the number of reverts made and received divided by group.

When a user performs a revert, thanks to the Wikimedia History Dumps, we can know the id of the
revision which is reverting but not the id of the reverted user. To solve this problem we
had to save the info in different dictionaries: \textit{reverters, editor, groups}, 
\bigskip

reverters[username] gives us the list of the revision it reverted. \\
\indent editor[revision\_id] gives us the user who performs that edit. \\
\indent groups[username] gives us the groups a user belongs.\\

Combining this dictionaries we have all the data necessary to compute all the metrics we need.
\begin{table}[H]
    \centering
    \ra{1.2}
    \begin{tabularx}{\columnwidth}{@{}ccc@{}}
        \midrule
        \textbf{user} & \textbf{group} & \textbf{year\_month} \\ \toprule
        carlos & adm & 2020-10  \\
        
         \bottomrule
    \end{tabularx}
    \begin{tabularx}{\columnwidth}{@{}XXXXXXXX@{}}
        \midrule
        \textbf{received} & \textbf{r\_reg}  & \textbf{r\_not} & \textbf{r\_adm} & \textbf{done} & \textbf{d\_reg} & \textbf{d\_not} & \textbf{d\_adm}\\ \toprule
        13 & 12 & 42  & 0 & 13 & 12 & 42  & 0  \\
        
         \bottomrule
    \end{tabularx}
    
    \caption{Entry of the mutual user TSV. \label{table:revks}}
\end{table}


The mutual revert analysis was harder to implement because in order to save the information about
mutual reverts we need the dataset sorted by pages, but to get the data monthly we should use the
one sorted by timestamp. We solved this problem by storing the user-page-month in the dataset, i.e.,
the information about the mutual reverts of a user in a specific month on a specific page.
This led to a larger dataset but with a higher level of information: it is easy to post-process it
grouping by user or month to have one entry per user or one entry per month, respectively. 


\begin{table}[H]
    \centering
    \ra{1.2}
    \begin{tabularx}{\columnwidth}{@{}XXXXXXX@{}}
        \midrule
        \textbf{user} & \textbf{group} & \textbf{page\_name}& \textbf{year\_month} & \textbf{mut\_adm}& \textbf{mut\_reg}& \textbf{mut\_not}\\ \toprule
        khalu & adm & Barcelona & 2020-10 & 13 & 12 & 4 \\
        
         \bottomrule
    \end{tabularx}

    
    \caption{Entry of the mutual user TSV. \label{table:rjevks}}
\end{table}







